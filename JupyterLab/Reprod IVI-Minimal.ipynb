{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, r2_score as r2\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "set_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sape(y_true, y_pred):\n",
    "    return 200*(np.abs(y_pred-y_true)/(np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "def relative_directive_error(y_true, y_pred, threshold):\n",
    "    y_true_sort = np.sort(y_true)\n",
    "    y_pred_sort = np.sort(y_pred)\n",
    "    y_true_threshold = np.abs(y_true_sort - threshold).argmin()\n",
    "    rel_error = np.abs(y_pred_sort[y_true_threshold]-y_true_sort[y_true_threshold])/threshold*100\n",
    "    return rel_error\n",
    "\n",
    "def rolling_day_agg(data, start, end, target, same_day=True):\n",
    "    if same_day:\n",
    "        comb = data[(data[\"zeit\"] >= start) & (data[\"zeit\"] <= end)]. \\\n",
    "                groupby(\"datum\", as_index=False).agg({target: \"mean\"})\n",
    "    else:\n",
    "        prev_day = data.loc[(data[\"zeit\"] >= start)].groupby(\"datum\", as_index=False).agg({target: [\"mean\", \"size\"]})\n",
    "        prev_day.columns = [\"_\".join(col_name).rstrip(\"_\") for col_name in prev_day.columns.to_flat_index()]\n",
    "        next_day = data.loc[(data[\"zeit\"] <= end)].groupby(\"datum\", as_index=False).agg({target: [\"mean\", \"size\"]})\n",
    "        next_day.columns = [\"_\".join(col_name).rstrip(\"_\") for col_name in next_day.columns.to_flat_index()]\n",
    "        \n",
    "        comb = prev_day.merge(next_day, on=\"datum\")\n",
    "        comb[[target+\"_mean_x\", target+\"_size_x\"]] = comb[[target+\"_mean_x\", target+\"_size_x\"]].shift(1)\n",
    "        comb[target] = (comb[target+\"_mean_x\"] * comb[target+\"_size_x\"] + comb[target+\"_mean_y\"] * comb[target+\"_size_y\"]) \\\n",
    "                       / (comb[target+\"_size_x\"] + comb[target+\"_size_y\"])\n",
    "        \n",
    "    return comb[[\"datum\", target]]\n",
    "\n",
    "def feature_engine(data):\n",
    "    # all times in UTC!\n",
    "    data_train = data.groupby(\"datum\", as_index=False).agg({\"pm\": \"mean\"})\n",
    "    data_train[\"pmshift\"] = data_train[\"pm\"].shift(1)\n",
    "\n",
    "    data_train[\"regen\"] = data.groupby(\"datum\", as_index=False).agg({\"regen\":\"mean\"})[\"regen\"]\n",
    "    data_train = data_train.merge(rolling_day_agg(data, \"07:00\", \"12:00\", \"regen\"), on=\"datum\", \n",
    "                                  how='left', suffixes=(\"\", \"1\"))\n",
    "    data_train = data_train.merge(rolling_day_agg(data, \"12:30\", \"16:00\", \"regen\"), on=\"datum\", \n",
    "                                  how='left', suffixes=(\"\", \"2\"))\n",
    "    data_train = data_train.merge(rolling_day_agg(data, \"16:30\", \"06:30\", \"regen\", same_day=False), \n",
    "                                  on=\"datum\", how='left', suffixes=(\"\", \"3\"))\n",
    "\n",
    "    data_train = data_train.merge(rolling_day_agg(data, \"05:00\", \"17:00\", \"windGe\"), on=\"datum\", \n",
    "                                  how='left', suffixes=(\"\", \"1\"))\n",
    "    data_train = data_train.merge(rolling_day_agg(data, \"16:30\", \"06:30\", \"windGe\", same_day=False), \n",
    "                                  on=\"datum\", how='left', suffixes=(\"\", \"2\"))\n",
    "    \n",
    "    temp1 = rolling_day_agg(data, \"05:00\", \"17:00\", \"temp\")\n",
    "    temp2 = rolling_day_agg(data, \"17:30\", \"04:30\", \"temp\", same_day=False)\n",
    "    temp1[\"tdiff1\"] = temp1[\"temp\"].diff()\n",
    "    temp2[\"tdiff2\"] = temp2[\"temp\"].diff()\n",
    "    \n",
    "    data_train = data_train.merge(temp1[[\"datum\", \"tdiff1\"]], on=\"datum\", how='left')\n",
    "    data_train = data_train.merge(temp2[[\"datum\", \"tdiff2\"]], on=\"datum\", how='left')\n",
    "    \n",
    "    maxtemp1 = data[(data[\"zeit\"].str.startswith(\"04:00\"))][[\"datum\", \"temp\"]]\n",
    "    mintemp2 = data[(data[\"zeit\"].str.startswith(\"12:00\"))][[\"datum\", \"temp\"]]\n",
    "    deltatemp = maxtemp1.merge(mintemp2, on=\"datum\", suffixes=(\"max\", \"min\"))\n",
    "    deltatemp[\"temp_grad\"] = deltatemp[\"tempmin\"] - deltatemp[\"tempmax\"]\n",
    "    \n",
    "    data_train = data_train.merge(deltatemp[[\"datum\", \"temp_grad\"]], on=\"datum\", how='left')\n",
    "    data_train[\"temp\"] = data.groupby(\"datum\", as_index=False).agg({\"temp\":\"mean\"})[\"temp\"]\n",
    "    \n",
    "    data_train[\"wochtag\"] = data.groupby([\"datum\"], as_index=False).agg({\"wochtag\": \"mean\"})[\"wochtag\"]\n",
    "    data_train = data_train.merge(data.groupby([\"wochtag\"], as_index=False).agg({\"pm\": \"mean\"}), \n",
    "                                  on=\"wochtag\", suffixes=(\"\", \"mean\"))\n",
    "    data_train[\"wochtag\"] = data_train[\"pmmean\"]\n",
    "    data_train.drop(columns=\"pmmean\", inplace=True)\n",
    "    data_train = data_train.sort_values(\"datum\")\n",
    "    \n",
    "    data_train = data_train.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return data_train\n",
    "\n",
    "def get_model(width = 64, depth = 2, loss=\"mean_squared_error\", len_input=32):\n",
    "    one_input = Input(shape=(len_input,), name='one_input')\n",
    "    #x = Dense(8, activation=\"relu\", kernel_initializer='uniform')(one_input)\n",
    "    x = Dense(width, activation=\"linear\", kernel_initializer='uniform')(one_input)\n",
    "    \n",
    "    for i in range(1,depth):\n",
    "        #width = max(8, int(width/2))\n",
    "        x = Dense(width, activation=\"relu\", kernel_initializer='uniform')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "    x = Dense(1, kernel_initializer='uniform', name=\"main_output\", activation=\"linear\")(x)\n",
    "    \n",
    "    model = Model(inputs=one_input, outputs=x)\n",
    "    model.compile(loss=loss, optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    \"pm25\": \"pm\"\n",
    "}\n",
    "\n",
    "data = {}\n",
    "data_valid = {}\n",
    "data_train = {}\n",
    "data_test = {}\n",
    "\n",
    "for dirpath,_ , files in os.walk(\"data-full/\"):\n",
    "    for f in files:\n",
    "        data_raw = pd.read_csv(dirpath + f)\n",
    "        data_raw[\"timestamp\"] = pd.to_datetime(data_raw[\"timestamp\"])\n",
    "        data_raw[\"zeit\"] = data_raw[\"timestamp\"].dt.time.astype(str)\n",
    "        data_raw[\"datum\"] = data_raw[\"timestamp\"].dt.date.astype(str)\n",
    "        data_raw[\"wochtag\"] = np.maximum(data_raw[\"timestamp\"].dt.weekday - 3, 1)\n",
    "        for sensor in data_raw[\"sensor\"].unique():\n",
    "            if \"train\" in f:\n",
    "                data[sensor] = data_raw[data_raw[\"sensor\"] == sensor].copy().rename(columns=columns)\n",
    "                data_train[sensor] = feature_engine(data[sensor])\n",
    "            else:\n",
    "                data_valid[sensor] = data_raw[data_raw[\"sensor\"] == sensor].copy().rename(columns=columns)\n",
    "                data_test[sensor] = feature_engine(data_valid[sensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNs\n",
    "train_predict = {}\n",
    "test_predict = {}\n",
    "\n",
    "for station in data_test.keys():\n",
    "    if True:\n",
    "        features = list(range(2,14))\n",
    "        X_train = data_train[station].iloc[:, features].values\n",
    "        y_train = data_train[station].loc[:, \"pm\"].values\n",
    "\n",
    "        X_test = data_test[station].iloc[:, features].values\n",
    "        y_test = data_test[station].loc[:, \"pm\"].values\n",
    "\n",
    "        model = get_model(10, 1, len_input=len(X_train[0]))\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train, shuffle=True, batch_size=16, epochs=200, verbose=0)\n",
    "        end = time.time() - start\n",
    "\n",
    "        mse_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "        mse_valid = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        train_predict[station] = model.predict(X_train)[:,0]\n",
    "        test_predict[station] = model.predict(X_test)[:,0]\n",
    "        \n",
    "        mae_train = mae(y_train, train_predict[station])\n",
    "        mae_valid = mae(y_test, test_predict[station])\n",
    "\n",
    "        smape_train = sape(y_train, train_predict[station]).mean()\n",
    "        smape_valid = sape(y_test, test_predict[station]).mean()\n",
    "        \n",
    "        r2_train = r2(y_train, train_predict[station])\n",
    "        r2_valid = r2(y_test, test_predict[station])\n",
    "        \n",
    "        rde_valid = relative_directive_error(y_test, test_predict[station], threshold=15)\n",
    "\n",
    "        #ps = permutation_importance(model, X_test, y_test, scoring=\"neg_mean_squared_error\")[\"importances_mean\"]\n",
    "\n",
    "        K.clear_session()\n",
    "        print(\"{}: MSE: {:.2f}/{:.2f} RMSE: {:.2f}/{:.2f} MAE: {:.2f}/{:.2f} SMAPE: {:.2f}%/{:.2f}% R²: {:.2f}/{:.2f} RDE: {:.2f}% in {:.1f}s\".format(\n",
    "                                                                    station, mse_train, mse_valid, \n",
    "                                                                    np.sqrt(mse_train), np.sqrt(mse_valid),\n",
    "                                                                    mae_train, mae_valid,\n",
    "                                                                    smape_train, smape_valid,\n",
    "                                                                    r2_train, r2_valid,\n",
    "                                                                    rde_valid, end))\n",
    "        #print(\" \".join([\"{}\".format(x) for x in data_train[station].columns[features]])) \n",
    "        #print(\"   \".join([\"{:.2f}\".format(y) for y in ps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_station = \"215\"\n",
    "plot_data = data_test[plot_station].copy()\n",
    "plot_data[\"datum\"] = pd.to_datetime(plot_data[\"datum\"])\n",
    "plot_data[\"pmp\"] = test_predict[plot_station]\n",
    "plot_data = pd.DataFrame(plot_data.resample(\"d\", on=\"datum\")[[\"datum\", \"pm\", \"pmp\"]].first())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,3))\n",
    "\n",
    "ax.plot(plot_data[\"pm\"], label=\"PM2.5\")\n",
    "ax.plot(plot_data[\"pmp\"], label=\"prediction\")\n",
    "ax.set_title(plot_station.capitalize(), size=16)\n",
    "ax.set_ylabel('PM2.5', size=20)\n",
    "ax.grid()\n",
    "\n",
    "ax.set_ylim(0, plot_data[\"pm\"].max() + 20)\n",
    "#ax.set_xlim(\"2021-09\", \"2022-04\")\n",
    "ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "for item in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    item.set_size(16)\n",
    "    \n",
    "ax.legend(fontsize=16, framealpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
