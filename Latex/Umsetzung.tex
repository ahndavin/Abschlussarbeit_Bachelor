\chapterpage\chapter{Umsetzung}
    이 장에서는 실험을 위해 선택된 데이터 세트의 구조 및 일명 Roh-Daten에 대한 전처리프로세스에 대해 설명하고 다음장에서는 3장에서 설명된 다섯 가지의 알고리즘의 구체적인 구현에 대해 설명한다. 마지막 장은 미세먼지 예측모델의 대략적인 구조에 대한 것이다.

    
    데이터 세트
        본 연구는 미세먼지 예측을 위해 A에서 제공되는 미세먼지 데이터세트를 사용한다. 이 기관은 x에 위치한 5개의 센서를 사용하여 해당 지역의 미세먼지농도 및 주변 기상측정값을 평균 10.3초 단위로 2020년 09월 01일부터 2022년 08월 31일까지의 기간동안 y개의 데이터를 수집했다. 이 데이터세트에는 총 네개의 features(날짜, 온도, 강수량, 바람세기)가 존재하며, 해당 독립변수와 종속변수의 단위 및 데이터타입은 표 1에 기술되어 있다. 이상치를 탐지하기 전에, 미세먼지농도와 강한 상관관계를 보이는 features들만 사용하기 위해 Pearson Correlation Coefficient (PCC)로 각각의 feature가 미세먼지농도 pm25와 어떤 관계를 갖고 있는지 분석하였다. 이는 두 변수간의 상관 관계를 계량화한 값으로, Cauchy-Schwarzsche Ungleichung에 의해 +1과 1 사이의 값을 갖는다. 일반적으로 PCC의 절댓값이 0.7 이상이면 강한 상관관계, 0.4 이상이면 뚜렷한 상관관계, 0.1 이상이면 약한 상관관계 그리고 0.1 미만이면 무시해도 좋을 상관 관계라고 해석된다. PCC로 데이터의 feature를 분석해 본 결과는 표 2와 같다.

        표1 표2
        
        따라서 피어슨 상관계수의 값이 0.4 이상인 pm25만을 최종 feature로 사용된다. 또한 중복데이터 x개, 음수데이터 y개와 '날짜'만 존재하고 다른 Variable은 없는 z개의 데이터가 실험에서 제외되었다.

        전체 데이터세트는 훈련데이터세트와 테스트데이터세트로 나뉘어있다. 이상치탐지 알고리즘을 위해서 이를 따로 나누지 않고 하나의 큰 데이터세트로 통합한다. 휴리스틱 관점에 따라 두 데이터세트 모두 이상치가 많은것으로 판단되었고 더 많은 데이터를 사용하여 이상치를 탐지하기 때문에 이상치 검출의 성능을 높일 수 있기 때문이다. 또 한가지 생각해 볼 것은 이상치탐지를 위해 각 센서를 따로 나누어서 탐지를 진행 할지, 모든 센서에 대해 한번에 탐지를 진행할지에 대한 것이다. 다시말해, Univariate Time Series Data로 사용할 것인지, Multivariate Time Series Data로 사용할 것인지이다. 이 Arbeit에서는 센서를 나눔으로써 해당 데이터 세트를 Univariate Time Series Data로 사용한다. Univariate Time Series Data는 Multivariate Time Series Data에서 자주 발생하는 Masking- und Swamping-Probleme을 피할수 있기 때문이다. 실제로 센서를 나누지 않고 실험 한 결과, 센서를 나누었을 때보다 SMAPE을 기준으로 평균 1.38\% 나쁜 성능을 보였다.

    알고리즘 구현
        Das Forschungsfeld entwickelt sich aufgrund seiner Robustheit, Offenheit, Benutzerfreundlichkeit und verfügbaren Open-Source-Material um Python herum. Daher wurde Python als Programmiersprache für die Umsetzung der in Abschnitt \ref{sec:Methoden} beschriebenen Methoden gewählt. Wie in Tabelle 4 gezeigt, wurde Open-Source-Material zur Implementierung von z-Score, K-Means, LOF und iForest verwendet und IQR wurde in Python selbst implementiert. Die für zusätzliche Arbeit verwendeten Bibliotheken sind in Tabelle 4.2 aufgelistet.
        
        표4, 표 4.2

        다음은 이 다섯가지 알고리즘의 하이퍼파라미터 설정 및 그들에게 데이터세트를 넘겨주기 전 그들의 성능을 높이기 위해 진행했던 작업에 대해 설명된다.

        IQR: Als IQR-Koeffizient wird der Wert $1,5$ gesetzt, der $\pm3\sigma$ in der Gaußschen Verteilung entspricht.

        z-Score: \cite{Killourhy09}에서 제안한 Threshold값인 1.96을 z-Score의 하이퍼파라미터로 사용됐다. 또한 이 알고리즘의 경우 Abschnitt \ref{sec:z-Score}에서 언급한 것처럼 해당 데이터세트가 가우시안 분포일 때 가장 좋은 성능을 얻기 때문에 Log-Funktion을 사용하여 해당 데이터세트의 분포를 변경했다. 그림 2는 종속변수 '$pm25$'의 분포와 probability에 대해 Log-Funktion을 이용한 변화를 보여준다. probability그래프에서 빨간 선 주변에 데이터가 많이 존재할수록 해당 데이터세트는 가우시안분포에 가깝다고 볼 수 있다. 마지막으로, Standardisierung을 하여 또 한번 알고리즘의 성능 향상을 도왔다.
        Als Hyperparameter von z-Score wurde das von \cite{Killourhy09} vorgeschlagene Threshold von $1,96$ verwendet. Außerdem wird für diesen Algorithmus, wie in Abschnitt \ref{sec:z-Score} erwähnt, die beste Leistung erzielt, wenn der Datensatz eine Gaußsche Verteilung hat, also wurde die Verteilung des Datensatzes mit der Log-Funktion geändert. Abbildung 2 zeigt die Änderung der Verteilungs und der Probability der abhängigen Variablen '$pm25$' mittels Log-Funktion. Je mehr Daten um die rote Linie im Probability-Diagramm vorhanden sind, desto näher liegt der Datensatz an der Gaußschen Verteilung. Schließlich wurde eine Standardisierung durchgeführt, die am Ende des Abschnittes \ref{sec:z-Score} beschrieben ist, um die Leistung des Algorithmus noch zu verbessern.

        K-Means: 그림 5는 Abschnitt \ref{sec:K-Means}에서 설명된 Elbow-Methode를 이용하여 해당 데이터세트에서 가장 적절한 K 값을 보여준다. 해당 메소드에 따르면 K=3일때가 가장 적절하다고 판단하지만 실제 하이퍼파라미터 K값은 휴리스틱방법을 통해 4로 설정했다. 이 알고리즘 또한 정규화를 필요로 했기에 이 또한 수행했다. 마지막으로 이 알고리즘은 원래 이상치탐지를 위해 고안된 알고리즘이 아니기 때문에 추가적으로 임계값을 설정하고 이 임계값을 초과하는 인스턴스를 제거해야 한다. 이 연구에서 고안된 방법은 Algorithmus 1과 같다.
        Abbildung 5 zeigt den am besten geeigneten $K$-Wert im Datensatz unter Verwendung der Elbow-Methode, die in Abschnitt \ref{sec:K-Means} beschrieben ist. Gemäß der Methode wird $K = 3$ als am geeignetsten angesehen, aber der eingestellte Wert des Hyperparameters $K$ wird durch ein heuristisches Verfahren auf $4$ gesetzt, wobei der Ergebniswert dieser Methode als Hinweis verwendet wird. Dieser Algorithmus erfordert auch eine Standardisierung, also wurde dies auch getan. Da dieser Algorithmus ursprünglich nicht für die Ausreißererkennung ausgelegt ist, muss schließlich zusätzlich ein Threshold gesetzt und Instanzen, die diesen Threshold überschreiten, entfernt werden. Die in dieser Arbeit verwendte Methode ist im Algorithmus 1.

        Algorithmus 1
        function entfernen_anomalie(D, c);
        input: D Datensatz und c Threshold(contamination)
        output: D

        Sortiere die Daten in Datensatz D nach a_score in der Reihenfolge des höchsten Werts
        (jeder Datei in diesem Datensatz hat eigenes a_score)

        while i==len(D)*c do:
            löschen(D_i)
            i++
        end

        return D

        임계값으로 c (contamination) 변수를 'entfernen_anomalie' 함수의 파라미터로 받고 전체 데이터세트 D의 인스턴스를 a_score를 기준으로 높은 값 순으로 정렬한 후 첫번째 인덱스부터 D * c번째 인덱스까지 제거한다.

        LOF: k값으로 \cite{Breunig00}에서 제안한 값 20을 사용했다. $LOF_k(p)$의 값은 휴리스틱 방법을 사용해 정수 2로 설정했다. 이 알고리즘 또한 데이터에 대한 표준화를 진행할 경우 더 좋은 성능을 보여주기 떄문에 표준화를 진행했다.

        iForest: scikit-learn에서 제공하는 Isolation Forest 알고리즘의 임계값은 위 K-Means에서 자체 정의한 임계값과 같은 방식을 사용한다. 따라서 휴리스틱 방법에 의해 contamination값을 0.04로 설정했다. 이 알고리즘은 표준화를 요구하지 않기 때문에 기존데이터를 그대로 사용한다.

        위의 모든 알고리즘의 마지막은 각각 성질에 최적화된 시각화를 제공한다. 또한 평균 10.3초 단위로 수집된 데이터를 한 시간 단위로 평균을 내어 변환함으로써 처리해야 하는 데이터의 양을 줄이고 각 데이터의 신빙성을 높였다.- Indem die in durchschnittlich jeden 10,3 Sekunden gesammelten Daten durch eine Mittelung um eine Stunde umgewandelt wird, kann die verarbeitende Datenmenge reduziert und die Zuverlässigkeit jeder Daten erhöht werden.



        Der oben beschriebene Code wurde auf Google Colab mit einem Intel(R) Xeon(R) und 12GB Arbeitsspeicher implementiert und getestet. In diesem System wurde Python in der Version 3.8.16 verwendet.

    모델 설명 짧게
        
        The function "feature_engine" is used to prepare the data for training a fine dust prediction model. The function takes in a DataFrame of fine dust data as input, and processes it by performing several operations on it. The processed data is returned as a new DataFrame, which can then be used to train the prediction model. The specific operations performed by the function include:

        - Grouping the data by date and calculating the mean fine dust level for each date.
        - Creating a new column "pmshift", which is a shifted version of the original fine dust level column.
        - Grouping the data by date and calculating the mean precipitation level for each date.
        - Using the "rolling_day_agg" function to calculate rolling averages of precipitation levels for different time periods of the day, and adding these columns to the DataFrame.
        - Merging the above columns with the original DataFrame.
        - Finally, It returns the processed DataFrame containing new features for training the model
        
        Shifting the values of the fine dust level column by one time step, allows the model to take into account the previous day's fine dust level when making predictions for the current day. This is because the fine dust levels on a given day may be influenced by the levels on the previous day. By including this information as a feature in the training data, the model can potentially make more accurate predictions.
        
        Splitting the day into different time periods and calculating rolling averages of precipitation levels for those periods is done to give the model more detailed information about how precipitation levels have changed over time. By providing the model with this information, it can potentially make more accurate predictions. The different time periods chosen for this function are "07:00" to "12:00", "12:30" to "16:00" and "16:00" to "20:00" . It's done to consider the effect of precipitation level on fine dust level during different time of the day. Precipitation in the morning may have a different effect on fine dust levels than precipitation in the afternoon or evening. By including this information as a feature in the training data, the model can potentially make more accurate predictions.
        
        Some possible additional questions that could be asked about this code include:
        How the regen column can be used in the model, and how it's related to the fine dust level?
        
        Regarding the answer:
        Regen column, which represents the precipitation level, can be used in the model as a feature, it's a factor affecting the fine dust level, by including this information as a feature in the training data, the model can potentially make more accurate predictions, because precipitation can affect the fine dust level.
        전체 데이터세트 중 학습데이터는 2020년 09월 01일부터 2021년 08월 31일까지 i개의 데이터이며 test데이터에는 2021년 09월 01일부터 2022년 08월 31일까지 j개의 데이터가 사용되었다. 5개의 센서중 하나는 학습데이터에만 존재하기 떄문에 평가단계에서는 제외됩니다.