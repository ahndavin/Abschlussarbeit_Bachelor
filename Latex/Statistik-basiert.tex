\chapterpage\chapter{Methoden zur Ausreißererkennung}
        Methoden und Umsetzung
    
        \section{Statistikbasierte Methoden zur Ausreißererkennung}
            Die statistikbasierte Ausreißererkennung ist eine frühe Methode der Ausreißererkennung. Die Definition eines anormalen Datens ist hier „ein Wert, der als teilweise oder vollständig unterschiedlich von der Wahrscheinlichkeitsverteilung der meisten Werten angesehen wird“ \cite{Anscombe60}. In diesem Kapitel wird zwei Methoden zur Erkennung von Ausreißern auf der Grundlage von Statistiken beschrieben.
            
            \subsection{Boxplot-Rule}
                Die Boxplot-Rule (Abbildung 9) ist die einfachste statistische Technik, die verwendet wird, um Ausreißer in univariaten und multivariaten Daten zu erkennen. Es verwendet Informationen wie unteres Quartil (Q1), Median (Median) und oberes Quartil (Q3), um diese Daten zu visualisieren.
                
                \begin{figure}[h]
                    \centering
                    \includegraphics[scale=0.25]{images/cat.jpg}
                    \caption{Ein Boxplot-Beispiel für univariate Daten}
                    \label{fig:IQR}
                \end{figure}
                
                Der für die Ausreißererkennung definierte Interquartile Range (IQR) ist die Differenz zwischen dem oberen Quartil (Q3) und dem unteren Quartil (Q1). Datenpunkte außerhalb des Bereichs zwischen $Q1-1,5*IQR$ und $Q3+1,5*IQR$ werden als Ausreißer erkannt. Als IQR-Koeffizient wird der Wert 1,5 eingestellt, da der oben berechnete Bereich $\pm3\sigma$ auf Gaußschen Daten entspricht, die 99,3\% der Beobachtungen abdecken \cite{Chandola09}. Die Formel in \ref{eqn:IQR} ist ein mathematischer Ausdruck von IQR, Obergrenze und Untergrenze.
                
                \begin{equation}
                    \label{eqn:IQR}
                    \begin{aligned}
                        \text{IQR} & = Q3 - Q1 \\
                        \text{Untergrenze} & = Q1 - 1.5 * IQR \\
                        \text{Obergrenze} & = Q3 + 1.5 * IQR
                    \end{aligned}
                \end{equation}
                
            \subsection{Z-Score}
                Der Z-Score ist eine häufig verwendete Metrik in der Statistik, die misst, wie weit ein beobachteter Wert vom Mittelwert entfernt ist. Im allgemeinen Fall wird es verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt. Die Gaußsche Verteilung wird auch als Normalverteilung bezeichnet und wenn die Datenpunkte glockenförmig verteilt sind, spricht man von einer Gaußschen Verteilung. Z-Score ist ein Wert, der misst, wie weit jeder Wert in dieser Gaußschen Verteilung vom Durchschnitt abweicht. Diese statistische Technik wird wie folgt unter Verwendung des beobachteten Werts, Mittelwerts und der Standardabweichung berechnet.

                Z-Score = (Beobachtungen - Mittelwert) / Standardabweichung, Diagramm

                Im Bereich der Ausreißererkennung wird ein Datenpunkt im Allgemeinen als Ausreißer definiert, wenn der Z-Score-Wert größer oder kleiner als $\pm1,96$ ist \cite{Killourhy09}. Dies liegt daran, dass die Datenpunkte außerhalb dieses Z-Score-Werts ungefähr 5\% des gesamten Datensatzes ausmachen.
                
                Wie oben erwähnt, „wird es im Allgemeinen verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt“, zeigen allgemeine statistische Techniken eine optimale Leistung, wenn sie auf einen Datensatz angewendet werden, der einer Gaußschen Verteilung folgt. Wenn ein Datensatz nicht der Gaußschen Verteilung folgt, wird er in eine Verteilung geändert, die der Gaußschen Verteilung nahe kommt, indem eine Log-Funktion auf den Datensatz angewendet wird, so dass allgemeine statistische Techniken angewendet werden können.
                    
                
        \section{Clustering-basierte Methoden zur Ausreißererkennung}
            Das Ziel des Daten-Clustering, auch bekannt als Cluster-Analyse, besteht darin, die natürliche(n) Gruppierung(en) einer Reihe von Mustern, Punkten oder Objekten zu entdecken \cite{Jain10}. Der cluster-basierte Algorithmus wird je nach Fall in drei Typen unterteilt und Ausreißer gemäß jedem Fall wie folgt definiert.

                1. Normalwerte gehören zu einem oder mehreren Clustern, Ausreißer gehören zu keinem Cluster.
                    Nachdem Cluster in den Datensatz gefunden und dazu gehörte Datenpunkte entfernt wurden, werden die verbleibenden Datenpunkte als Ausreißer behandelt.

                2. Bei geringem Abstand zum nächsten Schwerpunkt des Clusters handelt es sich um einen Normalwert, bei großem Abstand um einen Ausreißer.
                    Nachdem das Clustering durchgeführt wurde, wird der Abstand zwischen der Mitte eines Clusters und eine zu diesem Cluster gehörte Datenpunkt als „Ausreißerwert“ definiert.

                3. Normale Datenpunkte gehören zu großen oder dichten Clustern und Ausreißer gehören zu kleinen oder spärlichen Clustern.
                    Die Größe oder Dichte des Clusters ist ein Kriterium dafür, ob die dazu gehörte Datenpunkte sich um Ausreißer handeln oder nicht.

            In diesem Abschnitt wird der k-Means-Algorithmus für den zweiten Fall beschrieben.
            
            \subsection{k-means Clustering}
                K-Means-Clustering ist ein Clustering-Algorithmus, der jeden Datenpunkt dem von diesem Datenpunkt nächstgelegenen Cluster zuweist \cite{Lloyd82}. Als Hyperparameter sollten die maximale Anzahl der Iterationen $L$, die Toleranz $\epsilon$, die Anzahl der Cluster $K$ und der anfängliche Mittelpunktswert $\mu^{(0)}_j, j=1,...,K$ gesetzt werden. Der K-Means-Algorithmus arbeitet im folgenden Prozess.
                
                    1. Die maximale Anzahl von Iterationen $L$, Toleranz $\epsilon$, Anzahl von Clustern $K$ und beliebige Datenpunkte $\mu_j$ werden eingestellt. Diese Datenpunkte werden als anfängliche Schwerpunkte des Clusters festgelegt.
                    
                    2. Sei u(t) der Schwerpunkt eines Clusters im t-ten Schritt. Zunächst wird für alle Datenpunkte x_n der Abstand zu jedem Schwerpunkt $\mu_j$ berechnet. Jeder Datenpunkt gehört zu dem Cluster mit dem nächstgelegenen Schwerpunkt. Hier verwendet die Distanz die euklidische Distanz. Das heißt, wenn
                    ,
                    ist der Cluster von Datenpunkten x_n gleich k.

                    3. Um den Schwerpunkt jedes Clusters zu aktualisiert, wird der durchschnittliche Abstand der Datenpunkte innerhalb des Clusters berechnet. Der Schwerpunkt bei Iteration t+1 wird wie folgt aktualisiert:

                    4. Wenn t > L oder u-k, endet der Algorithmus, andernfalls geht der Algorithmus zurück zu Schritt 2 und wiederholt.
                

                Der K-Means-Clustering rekonstruiert kontinuierlich die Cluster des Datensatzes, bis jeder Schwerpunkt gegen einen bestimmten Wert konvergiert. Es konvergiert im Allgemeinen, wenn die Ähnlichkeit zwischen Clustern in einem Datensatz maximal ist.

                K-Means-Clustering은 실제 데이터세트가 어떤 군집 구조를 가지고 있는지 모르기 때문에, 적절한 K값을 선택하는 것이 중요하다. 데이터세트를 시각화 했을 때 직관적으로 군집의 개수를 인식할 수 있는 경우엔 휴리스틱 방법을 사용할 수 있지만, 그렇지 않은 일반적인 경우엔 데이터세트의 군집 개수를 추정할 수 있는 method를 사용한다. 또한 군집의 개수를 선택했으면 그 군집의 초기 중심점을 선택하는 것 또한 해당 알고리즘의 시간복잡도와 성능의 향상에 중요한 요소이다. 이 글에서는 군집의 개수를 추정할 수 있는 기법 중에 가장 자주 쓰이는 elbow method와 군집의 초기 중심점을 찾는 방법 중 하나인 K-Means++에 대해 설명된다.
                
                Die Elbowmethode ist eine Technik, die die SSE(sum of squred error) für jede Anzahl von Clustern berechnet und visualisiert und dann die Anzahl der Cluster auswählt, die dem Teil (elbow) entspricht, der eine sanfte Neigung zeigt, nachdem er eine steile Neigung gezeigt hat.

                
                \begin{equation}
                    \label{eqn:SSE}
                    \text{SSE}=\sum_{i=1}^{n}(y_i-\hat{y_i})^2    
                \end{equation}

                K-Means++의 Prozess는 아래와 같다:
                    1. Auswählen einen beliebigen Punkt aus den Datenpunkten. Dieser Punkt ist dann erster Schwerpunkt.
                    2. Jeder nicht ausgewählte Datenpunkt berechnet die Entfernung zum nächstgelegenen Schwerpunkt.
                    3. Der t-te Schwerpunkt wird gemäß der Wahrscheinlichkeit proportional zur Entfernung von jedem Punkt ausgewählt. Das heißt, ein Datenpunkt, der so weit wie möglich von einem bereits festgelegten Schwerpunkt entfernt platziert ist, wird als nächster Schwerpunkt festgelegt.
                    4. Wiederholen die Schritte 2 und 3, bis K Schwerpunkte ausgewählt wurden.

                K-Means++는 알고리즘은 기 값을 설정하기 위해 추가적인 시간을 필요로 하지만, 이후 선택된 초기 값은 이후 K-Means-Clustering이 $O(log K)$의 시간 동안 최적의 해를 찾는 것을 보장한다.
                
                

                kmeans를 포함한 클러스터링 알고리즘은 거리 기반 측정을 사용하여 데이터 포인트 간의 유사성을 결정하므로 기능마다 다른 측정 단위를 가진 데이터세트에는 평균이 0이고 표준 편차가 1이 되도록 데이터를 표준화하는 것이 좋다.
                
            \subsection{Density-Based Spatial Clustering of Applications with Noise}
                Density-based spatial clustering of applications with noise
                
        \section{Dichtebasierte Methoden zur Ausreißererkennung}
            Dichtebasierte Anomalieerkennungstechniken schätzen die Dichte der Nachbarschaft jedes Datenpunkts. Ein Datenpunkt, der in einer Nachbarschaft mit geringer Dichte liegt, wird als anomal deklariert, während einn Datenpunkt, der in einer dichten Nachbarschaft liegt, als normal deklariert wird.
            
            In den beiden folgenden Abschnitten werden zwei dichtebasierte Methoden, Local Outlier Factor und Isolation Forest, beschrieben.

            \subsection{Local Outlier Factor}
                LOF gibt Informationen darüber aus, wie weit jeder Datenpunkt von den anderen entfernt ist. Das wichtigste Merkmal von LOF besteht darin, Ausreißer nicht unter Berücksichtigung aller Daten als Ganzes zu beurteilen, sondern Ausreißer aus lokaler Sicht anhand von Daten rund um den Datenpunkt zu beurteilen. Zum intuitiven Verständnis ist ein Beispiel wie folgt:

                \begin{figure}[h] % muss eigenes Bild!
                    \centering \includegraphics[scale=0.5]{images/LOF_2d.png}
                    \caption{ein Datensatz in 2 Dimension \cite{Breunig00}}
                    \label{fig:LOF_2d}
                \end{figure}

                Hier existieren die Gruppe C_1 mit niedriger Dichte, die Gruppe C_2 mit hoher Dichte und die Ausreißer O_1 und O_2. Die meisten Algorithmen zur Erkennung von Ausreißern vergleichen jeden Datenpunkt mit den gesamten Daten, um festzustellen, ob es sich um einen Ausreißer handelt oder nicht. Solche Algorithmen berücksichtigen O_2 in der Abbildung\ref{fig:LOF_2d} nicht als Ausreißer. Da der Abstand zwischen C_2 und O_2 ähnlich dem Abstand zwischen Datenpunkten der Gruppe C_1 ist, ist es schwierig, O_2 insgesamt gesehen als Ausreißer zu betrachten. Um diese Nachteile zu überwinden, zielt LOF darauf ab, den Grad der Ausreißer unter Verwendung lokaler Informationen anzuzeigen.

            \subsection{Isolation Forest}
                Isolation Forest
                
            