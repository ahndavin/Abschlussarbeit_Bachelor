\chapterpage\chapter{Methoden zur Ausreißererkennung}
        Methoden und Umsetzung
    
        \section{Statistikbasierte Methoden zur Ausreißererkennung}
            Die statistikbasierte Ausreißererkennung ist eine frühe Methode der Ausreißererkennung. Die Definition eines anormalen Datens ist hier „ein Wert, der als teilweise oder vollständig unterschiedlich von der Wahrscheinlichkeitsverteilung der meisten Werten angesehen wird“ \cite{Anscombe60}. In diesem Kapitel wird zwei Methoden zur Erkennung von Ausreißern auf der Grundlage von Statistiken beschrieben.
            
            \subsection{Boxplot-Rule}
                Die Boxplot-Rule (Abbildung 9) ist die einfachste statistische Technik, die verwendet wird, um Ausreißer in univariaten und multivariaten Daten zu erkennen. Es verwendet Informationen wie unteres Quartil (Q1), Median (Median) und oberes Quartil (Q3), um diese Daten zu visualisieren.
                
                \begin{figure}[h]
                    \centering
                    \includegraphics[scale=0.25]{images/cat.jpg}
                    \caption{Ein Boxplot-Beispiel für univariate Daten}
                    \label{fig:IQR}
                \end{figure}
                
                Der für die Ausreißererkennung definierte Interquartile Range (IQR) ist die Differenz zwischen dem oberen Quartil (Q3) und dem unteren Quartil (Q1). Datenpunkte außerhalb des Bereichs zwischen $Q1-1,5*IQR$ und $Q3+1,5*IQR$ werden als Ausreißer erkannt. Als IQR-Koeffizient wird der Wert 1,5 eingestellt, da der oben berechnete Bereich $\pm3\sigma$ auf Gaußschen Daten entspricht, die 99,3\% der Beobachtungen abdecken \cite{Chandola09}. Die Formel in \ref{eqn:IQR} ist ein mathematischer Ausdruck von IQR, Obergrenze und Untergrenze.
                
                \begin{equation}
                    \label{eqn:IQR}
                    \begin{aligned}
                        \text{IQR} & = Q3 - Q1 \\
                        \text{Untergrenze} & = Q1 - 1.5 * IQR \\
                        \text{Obergrenze} & = Q3 + 1.5 * IQR
                    \end{aligned}
                \end{equation}
                
            \subsection{Z-Score}
                Der Z-Score ist eine häufig verwendete Metrik in der Statistik, die misst, wie weit ein beobachteter Wert vom Mittelwert entfernt ist. Im allgemeinen Fall wird es verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt. Die Gaußsche Verteilung wird auch als Normalverteilung bezeichnet und wenn die Datenpunkte glockenförmig verteilt sind, spricht man von einer Gaußschen Verteilung. Z-Score ist ein Wert, der misst, wie weit jeder Wert in dieser Gaußschen Verteilung vom Durchschnitt abweicht. Diese statistische Technik wird wie folgt unter Verwendung des beobachteten Werts, Mittelwerts und der Standardabweichung berechnet.

                Z-Score = (Beobachtungen - Mittelwert) / Standardabweichung, Diagramm

                Im Bereich der Ausreißererkennung wird ein Datenpunkt im Allgemeinen als Ausreißer definiert, wenn der Z-Score-Wert größer oder kleiner als $\pm1,96$ ist \cite{Killourhy09}. Dies liegt daran, dass die Datenpunkte außerhalb dieses Z-Score-Werts ungefähr 5\% des gesamten Datensatzes ausmachen.
                
                Wie oben erwähnt, „wird es im Allgemeinen verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt“, zeigen allgemeine statistische Techniken eine optimale Leistung, wenn sie auf einen Datensatz angewendet werden, der einer Gaußschen Verteilung folgt. Wenn ein Datensatz nicht der Gaußschen Verteilung folgt, wird er in eine Verteilung geändert, die der Gaußschen Verteilung nahe kommt, indem eine Log-Funktion auf den Datensatz angewendet wird, so dass allgemeine statistische Techniken angewendet werden können.
                    
                
        \section{Clustering-basierte Methoden zur Ausreißererkennung}
            Das Ziel des Daten-Clustering, auch bekannt als Cluster-Analyse, besteht darin, die natürliche(n) Gruppierung(en) einer Reihe von Mustern, Punkten oder Objekten zu entdecken \cite{Jain10}. Der cluster-basierte Algorithmus wird je nach Fall in drei Typen unterteilt und Ausreißer gemäß jedem Fall wie folgt definiert.

                1. Normalwerte gehören zu einem oder mehreren Clustern, Ausreißer gehören zu keinem Cluster.
                    Nachdem Cluster in den Datensatz gefunden und dazu gehörte Datenpunkte entfernt wurden, werden die verbleibenden Datenpunkte als Ausreißer behandelt.

                2. Bei geringem Abstand zum nächsten Schwerpunkt des Clusters handelt es sich um einen Normalwert, bei großem Abstand um einen Ausreißer.
                    Nachdem das Clustering durchgeführt wurde, wird der Abstand zwischen der Mitte eines Clusters und eine zu diesem Cluster gehörte Datenpunkt als „Ausreißerwert“ definiert.

                3. Normale Datenpunkte gehören zu großen oder dichten Clustern und Ausreißer gehören zu kleinen oder spärlichen Clustern.
                    Die Größe oder Dichte des Clusters ist ein Kriterium dafür, ob die dazu gehörte Datenpunkte sich um Ausreißer handeln oder nicht.

            In diesem Abschnitt wird der k-Means-Algorithmus für den zweiten Fall beschrieben.
            
            \subsection{k-means Clustering}
                K-Means-Clustering ist ein Clustering-Algorithmus, der jeden Datenpunkt dem von diesem Datenpunkt nächstgelegenen Cluster zuweist \cite{Lloyd82}. Als Hyperparameter sollten die maximale Anzahl der Iterationen $L$, die Toleranz $\epsilon$, die Anzahl der Cluster $K$ und der anfängliche Mittelpunktswert $\mu^{(0)}_j, j=1,...,K$ gesetzt werden. Der K-Means-Algorithmus arbeitet im folgenden Prozess.
                
                    1. Die maximale Anzahl von Iterationen $L$, Toleranz $\epsilon$, Anzahl von Clustern $K$ und beliebige Datenpunkte $\mu_j$ werden eingestellt. Diese Datenpunkte werden als anfängliche Schwerpunkte des Clusters festgelegt.
                    
                    2. Sei u(t) der Schwerpunkt eines Clusters im t-ten Schritt. Zunächst wird für alle Datenpunkte x_n der Abstand zu jedem Schwerpunkt $\mu_j$ berechnet. Jeder Datenpunkt gehört zu dem Cluster mit dem nächstgelegenen Schwerpunkt. Hier verwendet die Distanz die euklidische Distanz. Das heißt, wenn
                    ,
                    ist der Cluster von Datenpunkten x_n gleich k.

                    3. Um den Schwerpunkt jedes Clusters zu aktualisiert, wird der durchschnittliche Abstand der Datenpunkte innerhalb des Clusters berechnet. Der Schwerpunkt bei Iteration t+1 wird wie folgt aktualisiert:

                    4. Wenn t > L oder u-k, endet der Algorithmus, andernfalls geht der Algorithmus zurück zu Schritt 2 und wiederholt.
                

                Der K-Means-Clustering rekonstruiert kontinuierlich die Cluster des Datensatzes, bis jeder Schwerpunkt gegen einen bestimmten Wert konvergiert. Es konvergiert im Allgemeinen, wenn die Ähnlichkeit zwischen Clustern in einem Datensatz maximal ist.

                K-Means-Clustering은 실제 데이터세트가 어떤 군집 구조를 가지고 있는지 모르기 때문에, 적절한 K값을 선택하는 것이 중요하다. 데이터세트를 시각화 했을 때 직관적으로 군집의 개수를 인식할 수 있는 경우엔 휴리스틱 방법을 사용할 수 있지만, 그렇지 않은 일반적인 경우엔 데이터세트의 군집 개수를 추정할 수 있는 method를 사용한다. 또한 군집의 개수를 선택했으면 그 군집의 초기 중심점을 선택하는 것 또한 해당 알고리즘의 시간복잡도와 성능의 향상에 중요한 요소이다. 이 글에서는 군집의 개수를 추정할 수 있는 기법 중에 가장 자주 쓰이는 elbow method와 군집의 초기 중심점을 찾는 방법 중 하나인 K-Means++에 대해 설명된다.
                
                Die Elbowmethode ist eine Technik, die die SSE(sum of squred error) für jede Anzahl von Clustern berechnet und visualisiert und dann die Anzahl der Cluster auswählt, die dem Teil (elbow) entspricht, der eine sanfte Neigung zeigt, nachdem er eine steile Neigung gezeigt hat.

                
                \begin{equation}
                    \label{eqn:SSE}
                    \text{SSE}=\sum_{i=1}^{n}(y_i-\hat{y_i})^2    
                \end{equation}

                K-Means++의 Prozess는 아래와 같다:
                    1. Auswählen einen beliebigen Punkt aus den Datenpunkten. Dieser Punkt ist dann erster Schwerpunkt.
                    2. Jeder nicht ausgewählte Datenpunkt berechnet die Entfernung zum nächstgelegenen Schwerpunkt.
                    3. Der t-te Schwerpunkt wird gemäß der Wahrscheinlichkeit proportional zur Entfernung von jedem Punkt ausgewählt. Das heißt, ein Datenpunkt, der so weit wie möglich von einem bereits festgelegten Schwerpunkt entfernt platziert ist, wird als nächster Schwerpunkt festgelegt.
                    4. Wiederholen die Schritte 2 und 3, bis K Schwerpunkte ausgewählt wurden.

                K-Means++는 알고리즘은 기 값을 설정하기 위해 추가적인 시간을 필요로 하지만, 이후 선택된 초기 값은 이후 K-Means-Clustering이 $O(log K)$의 시간 동안 최적의 해를 찾는 것을 보장한다.
                
                

                kmeans를 포함한 클러스터링 알고리즘은 거리 기반 측정을 사용하여 데이터 포인트 간의 유사성을 결정하므로 기능마다 다른 측정 단위를 가진 데이터세트에는 평균이 0이고 표준 편차가 1이 되도록 데이터를 표준화하는 것이 좋다.
                
            \subsection{Density-Based Spatial Clustering of Applications with Noise}
                Density-based spatial clustering of applications with noise
                
        \section{Dichtebasierte Methoden zur Ausreißererkennung}
            Dichtebasierte Anomalieerkennungstechniken schätzen die Dichte der Nachbarschaft jedes Datenpunkts. Ein Datenpunkt, der in einer Nachbarschaft mit geringer Dichte liegt, wird als anomal deklariert, während einn Datenpunkt, der in einer dichten Nachbarschaft liegt, als normal deklariert wird. In den beiden folgenden Abschnitten werden zwei dichtebasierte Methoden, Local Outlier Factor und Isolation Forest, beschrieben.
            
            \subsection{Local Outlier Factor (LOF)}
                Local Outlier Factor gibt Informationen darüber aus, wie weit jeder Datenpunkt von den anderen entfernt ist. Das wichtigste Merkmal von LOF besteht darin, Ausreißer nicht unter Berücksichtigung aller Daten als Ganzes zu beurteilen, sondern Ausreißer aus lokaler Sicht anhand von Daten rund um den Datenpunkt zu beurteilen. Zum intuitiven Verständnis ist ein Beispiel wie folgt:

                \begin{figure}[h] % muss eigenes Bild!
                    \centering \includegraphics[scale=0.25]{images/LOF_2d.png}
                    \caption{Ein Datensatz in 2 Dimension \cite{Breunig00}}
                    \label{fig:LOF_2d}
                \end{figure}

                Hier existieren die Gruppe $C_1$ mit niedriger Dichte, die Gruppe $C_2$ mit hoher Dichte und die Ausreißer $O_1$ und $O_2$. Die meisten Algorithmen zur Erkennung von Ausreißern vergleichen jeden Datenpunkt mit den gesamten Daten, um festzustellen, ob es sich um einen Ausreißer handelt oder nicht. Solche Algorithmen berücksichtigen $O_2$ in der Abbildung \ref{fig:LOF_2d} nicht als Ausreißer. Da der Abstand zwischen $C_2$ und $O_2$ ähnlich dem Abstand zwischen Datenpunkten der Gruppe $C_1$ ist, ist es schwierig, $O_2$ insgesamt gesehen als Ausreißer zu betrachten. Um diese Nachteile zu überwinden, zielt LOF darauf ab, den Grad der Ausreißer unter Verwendung lokaler Informationen anzuzeigen. Die sechs mathematischen Definitionen, die in \cite{Breunig00} verwendet werden, sind eine gute Möglichkeit, die Funktionsweise des Algorithmus zu verstehen. In diesem Definition wird $o$, $p$, $q$ verwendet, um Datenpunkte in einem Datensatz $C$ zu bezeichnen.
                
                \begin{description}
                    \item[Definition 1:]{(die Distanz zwischen den Objekten $p$ und $q$)
                    
                        Hier wird die Notation $d(p, q)$ verwendet, um den Abstand zwischen Datenpunkte $p$ und $q$ zu bezeichnen.
                    }
                    
                    \item[Definition 2:]{($k$-distance eines Objekts $p$)
                    
                        $k$-distance($p$) ist die Distanz von einem bestimmten Datenpunkt $p$ zum $k$-ten nächsten Nachbarn.
                    }
                    
                    \item[Definition 3:]{($k$-distance Nachbarschaft eines Objekts $p$)
                    
                        Die Anzahl der in der $k$-distance($p$) enthaltenen Daten wird als $N_{k\text-distance(p)}(p)=\{q\in C \backslash \{p\}\mid d(p, q)\leq k\text-distance(p)\}$, ausgedrückt, und die Notation wird mit der Abkürzung $N_k(p)$ vereinfacht.
    
                        Man kann vielleicht so denken, dass der Wert von $N_k(p)$ $k$ ist, weil $k$ ausgewählt sind. Wenn der Abstand jedoch kontinuierlich ist, gibt es genau $3$ Nachbarn innerhalb des $3$-distance, aber wenn der Abstand diskret ist und es mehr als einen Datenpunkt im gleichen Abstand gibt, kann die Anzahl von $N_k(p)$ größer als $k$ sein.
                    }
                    
                    \item[Definition 4:]{(Erreichbarkeitsdistanz eines Objekts $p$ in Bezug auf Objekt $o$)
                    
                    Sei $k$ eine natürliche Zahl, dann definiert die Erreichbarkeitsdistanz von einem Datanpunkt $p$ zu einem anderem Datenpunkt $o$ als

                    \begin{equation}
                        \label{eqn:SSE}
                        reach \text-dist_k(p, o)=max\{k\text - distance(o), d(p, o)\}.
                    \end{equation}
                    
                    Abbildung 2 zeigt das Konzept der Erreichbarkeitsdistanz für $k = 3$. Wenn der Datenpunkt $p_2$ in Abbildung 2 weit von $o$ entfernt ist, ist die Erreichbarkeitsdistanz zwischen ihnen intuitiv einfach die tatsächliche Distanz. Wenn es jedoch "genug nah" ist, wie $p_1$ in Abbildung 2, wird die Distanz durch $k\text - distance(o)$ ersetzt. Diese Idee dient als Puffer, um sicherzustellen, dass der Wert von $d(p, o)$ für alle $p$ in der Nähe von $o$ nicht zu klein wird, da später die Dichte durch dieser $reach \text - distance$ miteinander verglichen wird. Die Stärke dieses Effekts kann durch den Parameter $k$ gesteuert werden.
                    }
                    
                    \item[Definition 5:]{(lokale Erreichbarkeitsdichte eines Objekts $p$)
                        Die lokale Erreichbarkeitsdichte (local reachability density, lrd) für den Datenpunkt p ist gegeben durch:
                        \begin{equation}
                            lrd_{k}(p) = \frac{|N_{k}(p)|}{\displaystyle \sum_{o \in N_{k}(p)}reach \text - dist_{k}(p, o)}
                        \end{equation}
                        Die Formel zeigt der Kehrwert des Mittelwerts von $reach \text - dist_{k}(p, o)$ für Datenpunkt $p$. Diese sagt uns aus, wie weit die Nachbarn um den Datenpunkt $p$ entfernt sind, also wie dicht sie um den Datenpunkt $p$  existieren.
                    }
                    
                    \item[Definition 6:]{((lokaler) Ausreißerfaktor eines Objekts $p$)
                    
                        $LOF_k(p)$ ist der Durchschnitt der Ratio(oder Verhältnis?) von Dichte $lrd_k(o), \; o \in N_k(p)$ und Dichte $lrd_k(p)$. Die Formel lautet wie folgt:

                        \begin{equation}
                            \label{eqn:LOF_k(p)}
                            LOF_k(p)=\frac{\displaystyle\sum_{o\in N_k(p)}{\frac{lrd_k(o)}{lrd_k(p)}}}{|N_k(p)|}
                        \end{equation}

                        Der Wert von $LOF_k(p)$ kann in drei Fälle unterteilt werden: (i) wenn $lrd_k(p)$ und $lrd_k(o)$ ähnlich sind, liegt die $LOF_k(p)$ nahe bei $1$; (ii) wenn $lrd_k(p)$ kleiner als $lrd_k(o)$ ist, die $LOF_k(p)$ ist größer als $1$; und (iii) wenn $lrd_k(p)$ größer als $lrd_k(o)$ ist, muss die $LOF_k(p)$ kleiner als $1$ sein. Da die Dichte zwischen den beiden Datenpunkten relativ verglichen wird, wird auf diese Weise die $reach \text-dist$, die als ein Puffer fungiert, in Definition 4 definiert. Andernfalls kann die Ratio bei Datenpunkten, die sich an extrem dichten Ort befinden, aufgrund winziger Unterschiede sehr empfindlich werden.

                        Datenpunkte mit einer geringeren lokalen Dichte als ihre Nachbarn werden als Ausreißer angenommen, daher kann der Wert größer als 1 als Threshold angewendet werden. Also, der Wert dieser $LOF_k(p)$ kann als "outlier score" angesehen werden, der angibt, wie nah p an einem Ausreißer liegt.
                    }
                \end{description}

                지금까지 LOF의 Funktionsweise에 대해 알아봤다. LOF의 장점으로는, 굉장히 밀집된 클러스터에서 조금만 떨어져 있어도 이상치로 탐지해준다는 점입니다. 단점은 하이퍼파라미터 값인 k를 몇으로 할지인 고질적인 문제가 있다. 그러나 경험적으로 k=10-20 \cite{Breunig00} 정도로 하는것이 좋다고한다. 그리고 threshold를 얼마로 잡아야할지를 알기 힘들다는 점이다. 어떤 데이터셋에서는 1.5이라는 값이 이상치이지만 어떤 데이터셋에서는 2.0라는 값을 가지고 있어도 정상 데이터일 수 있습니다. 최적의 알고리즘 성능을 얻기 위해 논문 A, B, C는 이를 해결하기 위한 방법에 대해 연구되었다. 또 다른 알고리즘 성능의 향상 방법은 섹션 \ref{sec:K-Means}의 K-Means의 경우와 같이 사용되는 데이터세트의 분포를 가우시안 분포로 바꾸는 방법이 있다.
                
            \subsection{Isolation Forest (iForest)}
                Isolation Forest 알고리즘을 구축한 논문 A의 저자는 이상치가 'few and different'한 데이터 인스턴스라고 정의한다. 이러한 속성은 이상 현상이 격리라는 메커니즘에 취약하다는 것을 보여준다. 해당 논문에서는 통계, 또는 거리나 밀도를 측정 하지 않고 순전히 격리 개념만으로 이상 징후를 탐지하는 Isolation Forest라는 방법을 제안했다 \cite{Liu08}. 해당 논문에서 격리(isolation)란 ‘인스턴스를 나머지 인스턴스와 분리하는 것’을 의미한다. 이상치일 수록 데이터인스턴스가 크게 벗어나 있기 때문에 Split하는 과정에서 빠르게 고립(격리와 같은 단어인지 확인!)된다. 이와 반대로 정상적인 인스턴스는 정상데이터 범위 내에 있기 때문에 Split과정을 여러번 거친 후에야 고립된다. 그림 1.1.a를 보면 이상치인 x_j는 정상데이터 범위에서 크게 벗어나 있기 때문에 몇번의 Split만으로도 쉽게 고립됨을 알 수 있다. 이와 반면에 그림 1.1.b의 x_i는 정상데이터 범위 내에 있기 때문에 많은 Split과정을 거쳐야 고립됨을 알 수 있다.
                그림 두개 \cite{Liu08}
                Isolation Tree는 위 관찰에 근거하여 고안되었다. 이 kartesische Koordinatenebene를 어떠한 규칙 없이 random split을 통해 partition 하는데, 이 partition으로 생긴 모든 sub-region들이 단 하나의 데이터 인스턴스를 가지고 있을 때까지 반복하여 split을 진행한다. 이때, 어떤 region에 단 하나의 데이터 인스턴스만 존재하게 되는 상황을 "isolation"이라고 부른다. iForest의 작동 과정을 간단하게 나타내면 다음과 같다.

                    1. Kartesische Koordinatenebene에 존재하는 모든 단일 인스턴스가 격리될 때까지 해당 공간을 무작위로 분할하는 binary Tree를 구성한다. 이 트리를 Isolation Tree라고 부른다.
                    2. 하위 샘플링된(bootstrapped) 각 데이터 세트에 대한 iTrees를 구축하고 iTree들의 앙상블인 iForest를 구축한다.
                    3. 각 데이터 인스턴스의 평균 경로 길이를 계산하고 뒤에서 정의할 anomaly score를 사용하여 이상 점수를 얻는다. 이상 점수가 높은 일부 데이터 인스턴스를 이상으로 식별한다.

                논문 A에서 정의된 세가지 정의는 이 세가지 단계의 목적을 이해할 수 있도록 도와준다.

                정의 1: (Isolation Tree (iTree))
                    Der Isolation Tree ist ein binärer Suchbaum. T_l sei ein externer Knoten ohne Kindknoten für den Knoten T des iForest und sei T_r ein interner Knoten mit genau zwei Kindknoten.

                   ㅣㅂ 이것의 노드 T에 대해 자식이 없는 외부 노드를 T_l, 정확히 두 개의 자식 노드를 갖고 있는 내부 노드를 T_r이라고 하자. n개의 인스턴스를 가진 데이터세트 X=(1, 2, n)에 대해 이 알고리즘은 iTree를 구성하기 위해 임의로 하나의 Feature를 선택한 다음 선택된 Feature의 최대값과 최소값 사이의 split-value p를 임의로 선택하여 다음의 조건을 만족할 때까지 X를 재귀적으로 분할한다: (i) 절대값 X=1 일 경우; (ii) X의 모든 데이터가 동일한 값을 가질 경우; and (iii) h(x) > e, h(x)는 Path Length of a data instance x in X, e는 제한 깊이. 위의 설명을 통해 iTree는 이 Tree의 각 노드가 정확히 0개 또는 2개의 자식 노드를 갖는 proper binary tree 모델이라는 것을 알 수 있으며, 이것이 이 알고리즘의 첫번째 아이디어이다.

                정의 2: (Isolation Forest)
                    정의 1의 방법은 단 하나의 iTree에서 나온 결과에 불과하기 때문에 h(x)값을 신뢰할 수 없고, 따라서 정규화된 h(x)의 값을 찾을 필요가 있다. 이 아이디어는 여러개의 iTree의 앙상블인 Isolation Forest로 구현된다. 서브섹션 5에서 설명된 Bagging 알고리즘을 활용하여 하나의 데이터세트를 여러 개의 sub-sample로 나누고 두 인스턴스 x_i, x_j가 포함된 모든 iTrees에서 해당 인스턴스들이 isolation될 때까지 소요된 split 횟수를 정규화한 그래프는 그림 3에서 확인할 수 있다.
                    그림 \cite{Liu08}
                    iTree의 개수가 늘어남에 따라 average path length가 수렴하기 때문에 ensemble하면 더 robust한 모형 만들 수 있는 이점이 생긴다.

                정의 3: (Anomaly score)
                    이상 탐지 방법에는 이상 점수가 필요하다. 이를 위해 해당 논문에서는 정의 1의 h(x)값과 iTree가 Binary Search Tree(BST)와 같은 구조라는 것을 이용한다. iTree는 BST와 같은 구조를 갖기 때문에 외부 노드에서의 종료의 상황은 BST에서 실패한 검색의 상황과 동일하다. 따라서 iTree의 평균 경로 길이를 추정하기 위해 BST에서 아이디어를 가져올 수 있다. X=(1, 2, n)가 주어졌을 때, 논문[9]는 BST에서 실패한 검색의 평균 경로 길이를 다음과 같이 제공한다.
                    
                        c(n) = 2H(n - 1) - (2(n - 1)/n)

                    여기서 H(i)는 Harmonic number이며 약 ln(i) + \gamma (Euler-Mascheroni-Konstante)의 값을 갖는다. 인스턴스 x의 이상 점수 s는 다음과 같이 정의됩니다.

                        $s(x,n) = 2^\frac{-E(h(x))}{c(n)}$

                    여기서 E(h(x))는 iTree들의 평균 h(x)입니다. 따라서 다음과 같은 조건이 성립합니다.
                        E(h(x)) → c(n) 이라면, s → 0.5
                        E(h(x)) → 0 이라면, s → 1
                        E(h(x)) → n-1 이라면, s → 0

                    s(x, n)을 이용해 E(h(x))의 값을 [0,1]로 표준화했다. E(h(x))가 클수록 anomaly score인 s(x,n)은 0에 가까워지고 E(h(x))가 작을수록 anomaly score인 s(x,n)은 1에 가까워지게 됩니다. 이 값을 기준으로 모든 데이터 포인트들의 anomaly score를 계산하여 서로 비교할 수 있게 됩니다.

                모든 인스턴스가 분리된다고 가정한다면 각 인스턴스는 외부 노드로 격리되며, 이 경우 외부 노드 수는 n개, 내부 노드 수는 n - 1 개로 구성됩니다. 따라서 총 노드 수는 2n - 1이며 Space Complexity 및 time complexity가 n과 선형적으로 증가한다는 것을 알 수 있습니다.

                Acuna04는 다변량 자료에서 다량의 이상치가 분포하는 경우 가면화 현상(Masking phenomenon) 및 수렁 현상(Swamping phenomenon)이 발생되며 이로 인해 이상치를 탐색하기 위한 성능이 저하된다고 밝힌 바 있다. 여기서, 가면화 현상이란 이상치가 군집화되어 있으면 정상치로 잘못 분류하는 현상이며, 수렁 현상이란 정상치가 이상치에 가까운 경우 정상치를 이상치로 잘못 분류하는 현상을 말한다. 하지만 해당 알고리즘은 전체 데이터를 bootstraping 프로세스를 거쳐서 사용되기 때문에 ‘Swamping’과 ‘Masking’ 문제를 예방할 수 있다.