\chapterpage\chapter{Methoden zur Ausreißererkennung}
        Methoden und Umsetzung
    
        \section{Statistikbasierte Methoden zur Ausreißererkennung}
            Die statistikbasierte Ausreißererkennung ist eine frühe Methode der Ausreißererkennung. Die Definition eines anormalen Datens ist hier „ein Wert, der als teilweise oder vollständig unterschiedlich von der Wahrscheinlichkeitsverteilung der meisten Werten angesehen wird“ \cite{Anscombe60}. In diesem Kapitel wird zwei Methoden zur Erkennung von Ausreißern auf der Grundlage von Statistiken beschrieben.
            
            \subsection{Boxplot-Rule}
                Die Boxplot-Rule (Abbildung 9) ist die einfachste statistische Technik, die verwendet wird, um Ausreißer in univariaten und multivariaten Daten zu erkennen. Es verwendet Informationen wie unteres Quartil (Q1), Median (Median) und oberes Quartil (Q3), um diese Daten zu visualisieren.
                
                \begin{figure}[h]
                    \centering
                    \includegraphics[scale=0.25]{images/cat.jpg}
                    \caption{Ein Boxplot-Beispiel für univariate Daten}
                    \label{fig:IQR}
                \end{figure}
                
                Der für die Ausreißererkennung definierte Interquartile Range (IQR) ist die Differenz zwischen dem oberen Quartil (Q3) und dem unteren Quartil (Q1). Datenpunkte außerhalb des Bereichs zwischen $Q1-1,5*IQR$ und $Q3+1,5*IQR$ werden als Ausreißer erkannt. Als IQR-Koeffizient wird der Wert 1,5 eingestellt, da der oben berechnete Bereich $\pm3\sigma$ auf Gaußschen Daten entspricht, die 99,3\% der Beobachtungen abdecken \cite{Chandola09}. Die Formel in \ref{eqn:IQR} ist ein mathematischer Ausdruck von IQR, Obergrenze und Untergrenze.
                
                \begin{equation}
                    \label{eqn:IQR}
                    \begin{aligned}
                        \text{IQR} & = Q3 - Q1 \\
                        \text{Untergrenze} & = Q1 - 1.5 * IQR \\
                        \text{Obergrenze} & = Q3 + 1.5 * IQR
                    \end{aligned}
                \end{equation}
                
            \subsection{Z-Score}
                Der Z-Score ist eine häufig verwendete Metrik in der Statistik, die misst, wie weit ein beobachteter Wert vom Mittelwert entfernt ist. Im allgemeinen Fall wird es verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt. Die Gaußsche Verteilung wird auch als Normalverteilung bezeichnet und wenn die Datenpunkte glockenförmig verteilt sind, spricht man von einer Gaußschen Verteilung. Z-Score ist ein Wert, der misst, wie weit jeder Wert in dieser Gaußschen Verteilung vom Durchschnitt abweicht. Diese statistische Technik wird wie folgt unter Verwendung des beobachteten Werts, Mittelwerts und der Standardabweichung berechnet.

                Z-Score = (Beobachtungen - Mittelwert) / Standardabweichung, Diagramm

                Im Bereich der Ausreißererkennung wird ein Datenpunkt im Allgemeinen als Ausreißer definiert, wenn der Z-Score-Wert größer oder kleiner als $\pm1,96$ ist \cite{Killourhy09}. Dies liegt daran, dass die Datenpunkte außerhalb dieses Z-Score-Werts ungefähr 5\% des gesamten Datensatzes ausmachen.
                
                Wie oben erwähnt, „wird es im Allgemeinen verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt“, zeigen allgemeine statistische Techniken eine optimale Leistung, wenn sie auf einen Datensatz angewendet werden, der einer Gaußschen Verteilung folgt. Wenn ein Datensatz nicht der Gaußschen Verteilung folgt, wird er in eine Verteilung geändert, die der Gaußschen Verteilung nahe kommt, indem eine Log-Funktion auf den Datensatz angewendet wird, so dass allgemeine statistische Techniken angewendet werden können.
                    
                
        \section{Clustering-basierte Methoden zur Ausreißererkennung}
            Das Ziel des Daten-Clustering, auch bekannt als Cluster-Analyse, besteht darin, die natürliche(n) Gruppierung(en) einer Reihe von Mustern, Punkten oder Objekten zu entdecken \cite{Jain10}. Der cluster-basierte Algorithmus wird je nach Fall in drei Typen unterteilt und Ausreißer gemäß jedem Fall wie folgt definiert.

                1. Normalwerte gehören zu einem oder mehreren Clustern, Ausreißer gehören zu keinem Cluster.
                    Nachdem Cluster in den Datensatz gefunden und dazu gehörte Datenpunkte entfernt wurden, werden die verbleibenden Datenpunkte als Ausreißer behandelt.

                2. Bei geringem Abstand zum nächsten Schwerpunkt des Clusters handelt es sich um einen Normalwert, bei großem Abstand um einen Ausreißer.
                    Nachdem das Clustering durchgeführt wurde, wird der Abstand zwischen der Mitte eines Clusters und eine zu diesem Cluster gehörte Datenpunkt als „Ausreißerwert“ definiert.

                3. Normale Datenpunkte gehören zu großen oder dichten Clustern und Ausreißer gehören zu kleinen oder spärlichen Clustern.
                    Die Größe oder Dichte des Clusters ist ein Kriterium dafür, ob die dazu gehörte Datenpunkte sich um Ausreißer handeln oder nicht.

            In diesem Abschnitt wird der k-Means-Algorithmus für den zweiten Fall beschrieben.
            
            \subsection{k-means Clustering}
                K-Means-Clustering ist ein Clustering-Algorithmus, der jeden Datenpunkt dem von diesem Datenpunkt nächstgelegenen Cluster zuweist \cite{Lloyd82}. Als Hyperparameter sollten die maximale Anzahl der Iterationen $L$, die Toleranz $\epsilon$, die Anzahl der Cluster $K$ und der anfängliche Mittelpunktswert $\mu^{(0)}_j, j=1,...,K$ gesetzt werden. Der K-Means-Algorithmus arbeitet im folgenden Prozess.
                
                    1. Die maximale Anzahl von Iterationen $L$, Toleranz $\epsilon$, Anzahl von Clustern $K$ und beliebige Datenpunkte $\mu_j$ werden eingestellt. Diese Datenpunkte werden als anfängliche Schwerpunkte des Clusters festgelegt.
                    
                    2. Sei u(t) der Schwerpunkt eines Clusters im t-ten Schritt. Zunächst wird für alle Datenpunkte x_n der Abstand zu jedem Schwerpunkt $\mu_j$ berechnet. Jeder Datenpunkt gehört zu dem Cluster mit dem nächstgelegenen Schwerpunkt. Hier verwendet die Distanz die euklidische Distanz. Das heißt, wenn
                    ,
                    ist der Cluster von Datenpunkten x_n gleich k.

                    3. Um den Schwerpunkt jedes Clusters zu aktualisiert, wird der durchschnittliche Abstand der Datenpunkte innerhalb des Clusters berechnet. Der Schwerpunkt bei Iteration t+1 wird wie folgt aktualisiert:

                    4. Wenn t > L oder u-k, endet der Algorithmus, andernfalls geht der Algorithmus zurück zu Schritt 2 und wiederholt.
                

                K-means 알고리즘은 중심점들이 각각 어떤 값에 수렴할 때까지 계속해서 데이터세트의 군집을 재구성한다. 이는 일반적으로 데이터세트의 군집 간 유사도가 최대가 될 때 수렴할 것이다.

                이 알고리즘의 문제점은
                kmeas++
                데이터 포인트 중에서 무작위로 균일하게 하나의 중심을 선택합니다.
                아직 선택되지 않은 각 데이터 포인트 x 에 대해 x 와 이미 선택된 가장 가까운 중심 사이의 거리인 D( x )를 계산합니다.
                점 x 가 D( x )^2 에 비례하는 확률로 선택 되는 가중 확률 분포를 사용하여 새 데이터 점 하나를 새 중심으로 무작위로 선택 합니다.
                k개의 센터가 선택 될 때까지 2단계와 3단계를 반복 합니다.
                초기 중심이 선택되었으므로 표준 k- 평균 클러스터링 을 사용하여 진행합니다.

                K-means 알고리즘은 실제 데이터세트가 어떤 군집 구조를 가지고 있는지 알고 있지 않기 때문에, 적절한 k값을 선택하는 것이 중요합니다. 일반적으로 데이터세트의 군집 개수를 추정할 수 있는 기준을 사용하거나, 여러 개의 k값을 시도해본 후 성능이 최대가 되는 k값을 선택하기도 합니다.
                

                kmeans를 포함한 클러스터링 알고리즘은 거리 기반 측정을 사용하여 데이터 포인트 간의 유사성을 결정하므로 기능마다 다른 측정 단위를 가진 데이터세트에는 평균이 0이고 표준 편차가 1이 되도록 데이터를 표준화하는 것이 좋습니다.
                
            \subsection{Density-Based Spatial Clustering of Applications with Noise}
                Density-based spatial clustering of applications with noise
                
        \section{Dichtebasierte Methoden zur Ausreißererkennung}
            Dichte-basiert
            
            \subsection{Isolation Forest}
                Isolation Forest
                
            \subsection{Local Outlier Factor}
                Local Outlier Factor
