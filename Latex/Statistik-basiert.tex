\chapterpage\chapter{Methoden zur Ausreißererkennung}
        Methoden und Umsetzung
    
        \section{Statistikbasierte Methoden zur Ausreißererkennung}
            Die statistikbasierte Ausreißererkennung ist eine frühe Methode der Ausreißererkennung. Die Definition eines anormalen Datens ist hier „ein Wert, der als teilweise oder vollständig unterschiedlich von der Wahrscheinlichkeitsverteilung der meisten Werten angesehen wird“ \cite{Anscombe60}. In diesem Kapitel wird zwei Methoden zur Erkennung von Ausreißern auf der Grundlage von Statistiken beschrieben.
            
            \subsection{Boxplot-Rule}
                Die Boxplot-Rule (Abbildung 9) ist die einfachste statistische Technik, die verwendet wird, um Ausreißer in univariaten und multivariaten Daten zu erkennen. Es verwendet Informationen wie unteres Quartil (Q1), Median (Median) und oberes Quartil (Q3), um diese Daten zu visualisieren.
                
                \begin{figure}[h]
                    \centering
                    \includegraphics[scale=0.25]{images/cat.jpg}
                    \caption{Ein Boxplot-Beispiel für univariate Daten}
                    \label{fig:IQR}
                \end{figure}
                
                Der für die Ausreißererkennung definierte Interquartile Range (IQR) ist die Differenz zwischen dem oberen Quartil (Q3) und dem unteren Quartil (Q1). Datenpunkte außerhalb des Bereichs zwischen $Q1-1,5*IQR$ und $Q3+1,5*IQR$ werden als Ausreißer erkannt. Als IQR-Koeffizient wird der Wert 1,5 eingestellt, da der oben berechnete Bereich $\pm3\sigma$ auf Gaußschen Daten entspricht, die 99,3\% der Beobachtungen abdecken \cite{Chandola09}. Die Formel in \ref{eqn:IQR} ist ein mathematischer Ausdruck von IQR, Obergrenze und Untergrenze.
                
                \begin{equation}
                    \label{eqn:IQR}
                    \begin{aligned}
                        \text{IQR} & = Q3 - Q1 \\
                        \text{Untergrenze} & = Q1 - 1.5 * IQR \\
                        \text{Obergrenze} & = Q3 + 1.5 * IQR
                    \end{aligned}
                \end{equation}
                
            \subsection{Z-Score}
                Der Z-Score ist eine häufig verwendete Metrik in der Statistik, die misst, wie weit ein beobachteter Wert vom Mittelwert entfernt ist. Im allgemeinen Fall wird es verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt. Die Gaußsche Verteilung wird auch als Normalverteilung bezeichnet und wenn die Datenpunkte glockenförmig verteilt sind, spricht man von einer Gaußschen Verteilung. Z-Score ist ein Wert, der misst, wie weit jeder Wert in dieser Gaußschen Verteilung vom Durchschnitt abweicht. Diese statistische Technik wird wie folgt unter Verwendung des beobachteten Werts, Mittelwerts und der Standardabweichung berechnet.

                Z-Score = (Beobachtungen - Mittelwert) / Standardabweichung, Diagramm

                Im Bereich der Ausreißererkennung wird ein Datenpunkt im Allgemeinen als Ausreißer definiert, wenn der Z-Score-Wert größer oder kleiner als $\pm1,96$ ist \cite{Killourhy09}. Dies liegt daran, dass die Datenpunkte außerhalb dieses Z-Score-Werts ungefähr 5\% des gesamten Datensatzes ausmachen.
                
                Wie oben erwähnt, „wird es im Allgemeinen verwendet, wenn der verwendete Datensatz einer Gaußschen Verteilung folgt“, zeigen allgemeine statistische Techniken eine optimale Leistung, wenn sie auf einen Datensatz angewendet werden, der einer Gaußschen Verteilung folgt. Wenn ein Datensatz nicht der Gaußschen Verteilung folgt, wird er in eine Verteilung geändert, die der Gaußschen Verteilung nahe kommt, indem eine Log-Funktion auf den Datensatz angewendet wird, so dass allgemeine statistische Techniken angewendet werden können.
                    
                
        \section{Clustering-basierte Methoden zur Ausreißererkennung}
            Das Ziel des Daten-Clustering, auch bekannt als Cluster-Analyse, besteht darin, die natürliche(n) Gruppierung(en) einer Reihe von Mustern, Punkten oder Objekten zu entdecken \cite{Jain10}. Der cluster-basierte Algorithmus wird je nach Fall in drei Typen unterteilt und Ausreißer gemäß jedem Fall wie folgt definiert.

                1. Normalwerte gehören zu einem oder mehreren Clustern, Ausreißer gehören zu keinem Cluster.
                    Nachdem Cluster in den Datensatz gefunden und dazu gehörte Datenpunkte entfernt wurden, werden die verbleibenden Datenpunkte als Ausreißer behandelt.

                2. Bei geringem Abstand zum nächsten Schwerpunkt des Clusters handelt es sich um einen Normalwert, bei großem Abstand um einen Ausreißer.
                    Nachdem das Clustering durchgeführt wurde, wird der Abstand zwischen der Mitte eines Clusters und eine zu diesem Cluster gehörte Datenpunkt als „Ausreißerwert“ definiert.

                3. Normale Datenpunkte gehören zu großen oder dichten Clustern und Ausreißer gehören zu kleinen oder spärlichen Clustern.
                    Die Größe oder Dichte des Clusters ist ein Kriterium dafür, ob die dazu gehörte Datenpunkte sich um Ausreißer handeln oder nicht.

            In diesem Abschnitt wird der k-Means-Algorithmus für den zweiten Fall beschrieben.
            
            \subsection{k-means Clustering}
                K-Means-Clustering ist ein Clustering-Algorithmus, der jeden Datenpunkt dem von diesem Datenpunkt nächstgelegenen Cluster zuweist \cite{Lloyd82}. Als Hyperparameter sollten die maximale Anzahl der Iterationen $L$, die Toleranz $\epsilon$, die Anzahl der Cluster $K$ und der anfängliche Mittelpunktswert $\mu^{(0)}_j, j=1,...,K$ gesetzt werden. Der K-Means-Algorithmus arbeitet im folgenden Prozess.
                
                    1. Die maximale Anzahl von Iterationen $L$, Toleranz $\epsilon$, Anzahl von Clustern $K$ und beliebige Datenpunkte $\mu_j$ werden eingestellt. Diese Datenpunkte werden als anfängliche Schwerpunkte des Clusters festgelegt.
                    
                    2. Sei u(t) der Schwerpunkt eines Clusters im t-ten Schritt. Zunächst wird für alle Datenpunkte x_n der Abstand zu jedem Schwerpunkt $\mu_j$ berechnet. Jeder Datenpunkt gehört zu dem Cluster mit dem nächstgelegenen Schwerpunkt. Hier verwendet die Distanz die euklidische Distanz. Das heißt, wenn
                    ,
                    ist der Cluster von Datenpunkten x_n gleich k.

                    3. Um den Schwerpunkt jedes Clusters zu aktualisiert, wird der durchschnittliche Abstand der Datenpunkte innerhalb des Clusters berechnet. Der Schwerpunkt bei Iteration t+1 wird wie folgt aktualisiert:

                    4. Wenn t > L oder u-k, endet der Algorithmus, andernfalls geht der Algorithmus zurück zu Schritt 2 und wiederholt.
                

                Der K-Means-Clustering rekonstruiert kontinuierlich die Cluster des Datensatzes, bis jeder Schwerpunkt gegen einen bestimmten Wert konvergiert. Es konvergiert im Allgemeinen, wenn die Ähnlichkeit zwischen Clustern in einem Datensatz maximal ist.

                K-Means-Clustering은 실제 데이터세트가 어떤 군집 구조를 가지고 있는지 모르기 때문에, 적절한 K값을 선택하는 것이 중요하다. 데이터세트를 시각화 했을 때 직관적으로 군집의 개수를 인식할 수 있는 경우엔 휴리스틱 방법을 사용할 수 있지만, 그렇지 않은 일반적인 경우엔 데이터세트의 군집 개수를 추정할 수 있는 method를 사용한다. 또한 군집의 개수를 선택했으면 그 군집의 초기 중심점을 선택하는 것 또한 해당 알고리즘의 시간복잡도와 성능의 향상에 중요한 요소이다. 이 글에서는 군집의 개수를 추정할 수 있는 기법 중에 가장 자주 쓰이는 elbow method와 군집의 초기 중심점을 찾는 방법 중 하나인 K-Means++에 대해 설명된다.
                
                Die Elbowmethode ist eine Technik, die die SSE(sum of squred error) für jede Anzahl von Clustern berechnet und visualisiert und dann die Anzahl der Cluster auswählt, die dem Teil (elbow) entspricht, der eine sanfte Neigung zeigt, nachdem er eine steile Neigung gezeigt hat.

                
                \begin{equation}
                    \label{eqn:SSE}
                    \text{SSE}=\sum_{i=1}^{n}(y_i-\hat{y_i})^2    
                \end{equation}

                K-Means++의 Prozess는 아래와 같다:
                    1. Auswählen einen beliebigen Punkt aus den Datenpunkten. Dieser Punkt ist dann erster Schwerpunkt.
                    2. Jeder nicht ausgewählte Datenpunkt berechnet die Entfernung zum nächstgelegenen Schwerpunkt.
                    3. Der t-te Schwerpunkt wird gemäß der Wahrscheinlichkeit proportional zur Entfernung von jedem Punkt ausgewählt. Das heißt, ein Datenpunkt, der so weit wie möglich von einem bereits festgelegten Schwerpunkt entfernt platziert ist, wird als nächster Schwerpunkt festgelegt.
                    4. Wiederholen die Schritte 2 und 3, bis K Schwerpunkte ausgewählt wurden.

                K-Means++는 알고리즘은 기 값을 설정하기 위해 추가적인 시간을 필요로 하지만, 이후 선택된 초기 값은 이후 K-Means-Clustering이 $O(log K)$의 시간 동안 최적의 해를 찾는 것을 보장한다.
                
                

                kmeans를 포함한 클러스터링 알고리즘은 거리 기반 측정을 사용하여 데이터 포인트 간의 유사성을 결정하므로 기능마다 다른 측정 단위를 가진 데이터세트에는 평균이 0이고 표준 편차가 1이 되도록 데이터를 표준화하는 것이 좋다.
                
            \subsection{Density-Based Spatial Clustering of Applications with Noise}
                Density-based spatial clustering of applications with noise
                
        \section{Dichtebasierte Methoden zur Ausreißererkennung}
            Dichtebasierte Anomalieerkennungstechniken schätzen die Dichte der Nachbarschaft jedes Datenpunkts. Ein Datenpunkt, der in einer Nachbarschaft mit geringer Dichte liegt, wird als anomal deklariert, während einn Datenpunkt, der in einer dichten Nachbarschaft liegt, als normal deklariert wird. In den beiden folgenden Abschnitten werden zwei dichtebasierte Methoden, Local Outlier Factor und Isolation Forest, beschrieben.
            
            \subsection{Local Outlier Factor (LOF)}
                Local Outlier Factor gibt Informationen darüber aus, wie weit jeder Datenpunkt von den anderen entfernt ist. Das wichtigste Merkmal von LOF besteht darin, Ausreißer nicht unter Berücksichtigung aller Daten als Ganzes zu beurteilen, sondern Ausreißer aus lokaler Sicht anhand von Daten rund um den Datenpunkt zu beurteilen. Zum intuitiven Verständnis ist ein Beispiel wie folgt:

                \begin{figure}[h] % muss eigenes Bild!
                    \centering \includegraphics[scale=0.25]{images/LOF_2d.png}
                    \caption{Ein Datensatz in 2 Dimension \cite{Breunig00}}
                    \label{fig:LOF_2d}
                \end{figure}

                Hier existieren die Gruppe $C_1$ mit niedriger Dichte, die Gruppe $C_2$ mit hoher Dichte und die Ausreißer $O_1$ und $O_2$. Die meisten Algorithmen zur Erkennung von Ausreißern vergleichen jeden Datenpunkt mit den gesamten Daten, um festzustellen, ob es sich um einen Ausreißer handelt oder nicht. Solche Algorithmen berücksichtigen $O_2$ in der Abbildung \ref{fig:LOF_2d} nicht als Ausreißer. Da der Abstand zwischen $C_2$ und $O_2$ ähnlich dem Abstand zwischen Datenpunkten der Gruppe $C_1$ ist, ist es schwierig, $O_2$ insgesamt gesehen als Ausreißer zu betrachten. Um diese Nachteile zu überwinden, zielt LOF darauf ab, den Grad der Ausreißer unter Verwendung lokaler Informationen anzuzeigen. Die sechs mathematischen Definitionen, die in \cite{Breunig00} verwendet werden, sind eine gute Möglichkeit, die Funktionsweise des Algorithmus zu verstehen. In diesem Definition wird $o$, $p$, $q$ verwendet, um Datenpunkte in einem Datensatz $C$ zu bezeichnen.
                
                \begin{description}
                    \item[Definition 1:]{(die Distanz zwischen den Objekten $p$ und $q$)
                    
                        Hier wird die Notation $d(p, q)$ verwendet, um den Abstand zwischen Datenpunkte $p$ und $q$ zu bezeichnen.
                    }
                    
                    \item[Definition 2:]{($k$-distance eines Objekts $p$)
                    
                        $k$-distance($p$) ist die Distanz von einem bestimmten Datenpunkt $p$ zum $k$-ten nächsten Nachbarn.
                    }
                    
                    \item[Definition 3:]{($k$-distance Nachbarschaft eines Objekts $p$)
                    
                        Die Anzahl der in der $k$-distance($p$) enthaltenen Daten wird als $N_{k\text-distance(p)}(p)=\{q\in C \backslash \{p\}\mid d(p, q)\leq k\text-distance(p)\}$, ausgedrückt, und die Notation wird mit der Abkürzung $N_k(p)$ vereinfacht.
    
                        Man kann vielleicht so denken, dass der Wert von $N_k(p)$ $k$ ist, weil $k$ ausgewählt sind. Wenn der Abstand jedoch kontinuierlich ist, gibt es genau $3$ Nachbarn innerhalb des $3$-distance, aber wenn der Abstand diskret ist und es mehr als einen Datenpunkt im gleichen Abstand gibt, kann die Anzahl von $N_k(p)$ größer als $k$ sein.
                    }
                    
                    \item[Definition 4:]{(Erreichbarkeitsdistanz eines Objekts $p$ in Bezug auf Objekt $o$)
                    
                    Sei $k$ eine natürliche Zahl, dann definiert die Erreichbarkeitsdistanz von einem Datanpunkt $p$ zu einem anderem Datenpunkt $o$ als

                    \begin{equation}
                        \label{eqn:SSE}
                        reach \text-dist_k(p, o)=max\{k\text - distance(o), d(p, o)\}.
                    \end{equation}
                    
                    Abbildung 2 zeigt das Konzept der Erreichbarkeitsdistanz für $k = 3$. Wenn der Datenpunkt $p_2$ in Abbildung 2 weit von $o$ entfernt ist, ist die Erreichbarkeitsdistanz zwischen ihnen intuitiv einfach die tatsächliche Distanz. Wenn es jedoch "genug nah" ist, wie $p_1$ in Abbildung 2, wird die Distanz durch $k\text - distance(o)$ ersetzt. Diese Idee dient als Puffer, um sicherzustellen, dass der Wert von $d(p, o)$ für alle $p$ in der Nähe von $o$ nicht zu klein wird, da später die Dichte durch dieser $reach \text - distance$ miteinander verglichen wird. Die Stärke dieses Effekts kann durch den Parameter $k$ gesteuert werden.
                    }
                    
                    \item[Definition 5:]{(lokale Erreichbarkeitsdichte eines Objekts $p$)
                        Die lokale Erreichbarkeitsdichte (local reachability density, lrd) für den Datenpunkt p ist gegeben durch:
                        \begin{equation}
                            lrd_{k}(p) = \frac{|N_{k}(p)|}{\displaystyle \sum_{o \in N_{k}(p)}reach \text - dist_{k}(p, o)}
                        \end{equation}
                        Die Formel zeigt der Kehrwert des Mittelwerts von $reach \text - dist_{k}(p, o)$ für Datenpunkt $p$. Diese sagt uns aus, wie weit die Nachbarn um den Datenpunkt $p$ entfernt sind, also wie dicht sie um den Datenpunkt $p$  existieren.
                    }
                    
                    \item[Definition 6:]{((lokaler) Ausreißerfaktor eines Objekts $p$)
                    
                        $LOF_k(p)$ ist der Durchschnitt der Ratio(oder Verhältnis?) von Dichte $lrd_k(o), \; o \in N_k(p)$ und Dichte $lrd_k(p)$. Die Formel lautet wie folgt:

                        \begin{equation}
                            \label{eqn:LOF_k(p)}
                            LOF_k(p)=\frac{\displaystyle\sum_{o\in N_k(p)}{\frac{lrd_k(o)}{lrd_k(p)}}}{|N_k(p)|}
                        \end{equation}

                        Der Wert von $LOF_k(p)$ kann in drei Fälle unterteilt werden: (i) wenn $lrd_k(p)$ und $lrd_k(o)$ ähnlich sind, liegt die $LOF_k(p)$ nahe bei $1$; (ii) wenn $lrd_k(p)$ kleiner als $lrd_k(o)$ ist, die $LOF_k(p)$ ist größer als $1$; und (iii) wenn $lrd_k(p)$ größer als $lrd_k(o)$ ist, muss die $LOF_k(p)$ kleiner als $1$ sein. Da die Dichte zwischen den beiden Datenpunkten relativ verglichen wird, wird auf diese Weise die $reach \text-dist$, die als ein Puffer fungiert, in Definition 4 definiert. Andernfalls kann die Ratio bei Datenpunkten, die sich an extrem dichten Ort befinden, aufgrund winziger Unterschiede sehr empfindlich werden.

                        Datenpunkte mit einer geringeren lokalen Dichte als ihre Nachbarn werden als Ausreißer angenommen, daher kann der Wert größer als 1 als Threshold angewendet werden. Also, der Wert dieser $LOF_k(p)$ kann als "outlier score" angesehen werden, der angibt, wie nah p an einem Ausreißer liegt.
                    }
                \end{description}

                지금까지 LOF의 Funktionsweise에 대해 알아봤다. LOF의 장점으로는, 굉장히 밀집된 클러스터에서 조금만 떨어져 있어도 이상치로 탐지해준다는 점입니다. 단점은 하이퍼파라미터 값인 k를 몇으로 할지인 고질적인 문제가 있다. 그러나 경험적으로 k=10-20 \cite{Breunig00} 정도로 하는것이 좋다고한다. 그리고 threshold를 얼마로 잡아야할지를 알기 힘들다는 점이다. 어떤 데이터셋에서는 1.5이라는 값이 이상치이지만 어떤 데이터셋에서는 2.0라는 값을 가지고 있어도 정상 데이터일 수 있습니다. 최적의 알고리즘 성능을 얻기 위해 논문 A, B, C는 이를 해결하기 위한 방법에 대해 연구되었다. 또 다른 알고리즘 성능의 향상 방법은 섹션 \ref{sec:K-Means}의 K-Means의 경우와 같이 사용되는 데이터세트의 분포를 가우시안 분포로 바꾸는 방법이 있다.
                
            \subsection{Isolation Forest}
                Isolation Forest
                
            